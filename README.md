# Overview

Simple Python demo to show how the Applog class can be used to augment Python's 
built-in logging package such that a multi-process application can be conveniently
logged to a single file.

It is possible that the built-in logging module in Python 3.x now does this 
internally; Wasatch used this approach when designing ENLIGHTEN on an earlier 
Python 2.7 baseline, and has not yet sought a replacement.

# Invocation

    $ python main.py --help
    usage: main.py [-h] [--logfile LOGFILE] [--stdout] [--log-level LOG_LEVEL]
                   [--timeout-sec TIMEOUT_SEC]

    Simple demonstration of Applog in multi-process application

    optional arguments:
      -h, --help            show this help message and exit
      --logfile LOGFILE     path to logfile
      --stdout              also log to stdout
      --log-level LOG_LEVEL
                            log level (DEBUG, INFO, WARN, CRITICAL)
      --timeout-sec TIMEOUT_SEC
                            how long logger should wait before exiting

See the included "sample.log" for a sample logfile generated.

# Theory of Operation

Instantiate Applog, providing any or all of these parameters (or none as they all 
have defaults):

    applog = Applog(
        log_level     = args.log_level, 
        pathname      = args.logfile, 
        enable_stdout = args.stdout,
        timeout_sec   = args.timeout_sec)

Persist that object somewhere that it doesn't go out of scope or get deleted for 
the lifetime of the application.  Assigning it to a global variable would be fine, 
but it doesn't need to be visible, just "referenced" so it doesn't get 
garbage-collected.

After creating that object, any of your classes or files that want to generate 
log lines just need to do this at the top of the file:

	import logging
	log = logging.getLogger(__name__)

What that does is use Python's built-in "logging" module to return a logger 
object with file scope, such that it can be used by any class, method or function 
within that file.  The parameter \_\_name\_\_ in the call to getLogger() means 
that log lines generated by that logger will all have the name of the file in the 
line.  For instance, if those lines are in Worker.py, then each log line generated 
from that file will contain the name "Worker" prefixed to the line.

(Let's call this moment in time "Applog instantiation"...I'll come back to this 
in a minute.)

So at this point, we are able to log anything and everything in the main process.

There are only two more steps to make this work in a multi-process application.  
First, you need to pass applog.log_queue to whatever class or files you have 
which fork new processes, all the way into the new process.  

Second, you need to then call Applog.configure_process to configure the "logging" 
module in your new process to automatically feed log messages into the 
multiprocessing.queue you just passed along.

At this point, every call to the built-in "logging" module in the new process 
will automatically send the debug message and any arguments to the Applog 
background process we created at "Applog instantiation."

How does this work?  When you first instantiated Applog, it automatically forked 
itself and created a background process.  Before doing that, it created a 
multiprocessing.Queue, which has two ends: a "parent" end into which log messages 
can be inserted, and a "child" end where the log messages come out, and get 
written to a file, printed to stdout, whatever.

It's key to recognize that you can copy the "parent" end of a multiprocessing.Queue 
as many times as you like; each parent will still feed to the same "child", off 
in its own hidden process (like Scion).  So each time we create a new background 
process, we pass the applog.log_queue into the new process.  The first thing we 
do in each new process is call Applog.configure_process(log_queue), which sets the 
"hooks and handles" of the built-in logging infrastructure to point to that remote 
process.

So really there are just a couple things to do:

1. instantiate Applog
2. pass a handle to applog.log_queue to any new processes
3. call configure_process(log_queue) every time a new process is created
4. add "import logging ; log = logging.getLogger(\_\_name\_\_)" at the top of 
   every file you want to debug

It's a good idea to call applog.close() when your application's done, but there's 
a timeout so it should close itself even if you don't remember.

# Notes and Caveats

## Portability

I get some duplicate log lines from background processes on my Mac under Python 3.7.

## Timing

It is not 100% guaranteed that the order of messages in the logfile will match the
the order in which they were generated, especially in multi-process environments.
Sort the logfile to be guaranteed of sequence (the timestamps should be correct).

## Exceptions

The Worker math is deliberately designed to occasionally throw exceptions, so
you can see how those appear in the logfile.  If you are in an 'except' block and
want to log the current exception, just add "exc_info=1" to your log line:

    try:
        might_throw()
    except:
        log.debug("caught exception", exc_info=1)

## Object arguments

If you send an object as an argument to the logger, like this:

    log.debug("received response: %s", response)

Note several key things about the object reference:

- It will be pickled (serialized) and sent to the logger process.  
- If this is a very large and complex object, this can be a heavy operation.  
- If this object cannot be pickled, you will generate an exception.  
- If some other process MODIFIES the CONTENTS of the referenced object AFTER you
  issue the log statement, but BEFORE it is pickled, you may find different data 
  in the log than does not accurately reflect the state at the time the log message 
  was executed.

For all of these reasons, if the object you are logging is in any way non-trivial,
you are recommended to stringify it FIRST, before passing it to the logger:

    log.debug("received response: %s", str(response))

## Timeouts

The implementation provided contains a timeout (default of 5sec), after which the
logger process will automatically kill itself if no new messages have been logged.
If you don't want this, set the timeout to zero, or use a background thread to 
log "heartbeat" messages once a second or whatever.  The timeout was added to 
handle the case in which the main application had unexpectedly crashed, and was
therefore leaving "hung processes" running that didn't have the sense to shut
themselves down.

# Provenance

The classes in this repository are simplified examples from the following ENLIGHTEN 
sources, if you want to trace them back to their more-complex parents:

| ApplogDemo    | Repository | File                            |
|---------------|------------|---------------------------------|
| Applog.py     | Wasatch.PY | wasatch/applog.py               |
| Worker.py     | Wasatch.PY | wasatch/WasatchDeviceWrapper.py |
| main.py       | ENLIGHTEN  | scripts/Enlighten.py            |
| SampleApp.py  | ENLIGHTEN  | enlighten/Controller.py         |

# History

- 2019-09-30 0.0.2
	- docs
- 2019-09-28 0.0.1
    - initial version
